rule_files:
  - ../../monitoring/base/victoriametrics/rules/converted/rook-alertrule.yaml

tests:
  - interval: 1m
    input_series:
      - series: 'up{job="rook",namespace="ceph-hdd"}'
        values: '1+0x15'
      - series: 'up{job="rook",namespace="ceph-ssd"}'
        values: '1+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: CephHddIsDown
        exp_alerts: []
      - eval_time: 15m
        alertname: CephSsdIsDown
        exp_alerts: []
  - interval: 1m
    input_series:
      - series: 'up{job="rook",namespace="ceph-hdd"}'
        values: '0+0x15'
      - series: 'up{job="rook",namespace="ceph-ssd"}'
        values: '1+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: CephHddIsDown
        exp_alerts:
          - exp_labels:
              severity: critical
              category: storage
            exp_annotations:
              runbook: Please consider to find root causes, and solve the problems
              summary: Rook/Ceph ceph-hdd's metrics can not be got.
      - eval_time: 15m
        alertname: CephSsdIsDown
        exp_alerts: []
  - interval: 1m
    input_series:
      - series: 'up{job="rook",namespace="ceph-ssd"}'
        values: '1+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: CephHddIsDown
        exp_alerts:
          - exp_labels:
              severity: critical
              category: storage
            exp_annotations:
              runbook: Please consider to find root causes, and solve the problems
              summary: Rook/Ceph ceph-hdd's metrics can not be got.
      - eval_time: 15m
        alertname: CephSsdIsDown
        exp_alerts: []
  - interval: 1m
    input_series:
      - series: 'up{job="rook",namespace="ceph-hdd"}'
        values: '1+0x15'
      - series: 'up{job="rook",namespace="ceph-ssd"}'
        values: '0+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: CephHddIsDown
        exp_alerts: []
      - eval_time: 15m
        alertname: CephSsdIsDown
        exp_alerts:
          - exp_labels:
              severity: critical
              category: storage
            exp_annotations:
              runbook: Please consider to find root causes, and solve the problems
              summary: Rook/Ceph ceph-ssd's metrics can not be got.

  - interval: 1m
    input_series:
      - series: 'ceph_health_status{job="rook",namespace="ceph-hdd"}'
        values: '0+0x15'
      - series: 'ceph_health_status{job="rook",namespace="ceph-ssd"}'
        values: '0+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: RookCephStatusIsError
        exp_alerts: []
  - interval: 1m
    input_series:
      - series: 'ceph_health_status{job="rook",namespace="ceph-hdd"}'
        values: '1+0x15'
      - series: 'ceph_health_status{job="rook",namespace="ceph-ssd"}'
        values: '0+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: RookCephStatusIsError
        exp_alerts: []
  - interval: 1m
    input_series:
      - series: 'ceph_health_status{job="rook",namespace="ceph-hdd"}'
        values: '2+0x15'
      - series: 'ceph_health_status{job="rook",namespace="ceph-ssd"}'
        values: '0+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: RookCephStatusIsError
        exp_alerts:
          - exp_labels:
              severity: critical
              category: storage
              job: rook
              namespace: ceph-hdd
            exp_annotations:
              runbook: Please consider to find root causes, and solve the problems
              summary: Rook/Ceph ceph-hdd's status is HEALTH_ERR.
  - interval: 1m
    input_series:
      - series: 'ceph_health_status{job="rook",namespace="ceph-hdd"}'
        values: '0+0x15'
      - series: 'ceph_health_status{job="rook",namespace="ceph-ssd"}'
        values: '3+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: RookCephStatusIsError
        exp_alerts:
          - exp_labels:
              severity: critical
              category: storage
              job: rook
              namespace: ceph-ssd
            exp_annotations:
              runbook: Please consider to find root causes, and solve the problems
              summary: Rook/Ceph ceph-ssd's status is HEALTH_ERR.
  # This test confirms that CephOSDIsNearLyNearFul is out because the metrics is above threshold.
  - interval: 1m
    input_series:
      - series: 'ceph_osd_stat_bytes{job="rook",namespace="ceph-hdd",ceph_daemon="osd.1"}'
        values: '100+0x15'
      - series: 'ceph_osd_stat_bytes_used{job="rook",namespace="ceph-hdd",ceph_daemon="osd.1"}'
        values: '81+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: CephOSDIsNearlyNearFull
        exp_alerts:
          - exp_labels:
              severity: warning
              job: rook
              namespace: ceph-hdd
              ceph_daemon: osd.1
            exp_annotations:
              summary: "Rook/Ceph osd.1 in ceph-hdd is used more than 80%."
              runbook: Please consider to find root causes, and solve the problems
  # This test confirms that CephOSDIsNearLyNearFul is not out because the metrics is below threshold.
  - interval: 1m
    input_series:
      - series: 'ceph_osd_stat_bytes{job="rook",namespace="ceph-hdd",ceph_daemon="osd.1"}'
        values: '100+0x15'
      - series: 'ceph_osd_stat_bytes_used{job="rook",namespace="ceph-hdd",ceph_daemon="osd.1"}'
        values: '80+0x15'
    alert_rule_test:
      - eval_time: 15m
        alertname: CephOSDIsNearlyNearFull
        exp_alerts: []
